# Speculative Tool Calling

Reduce LLM agent latency by speculatively pre-executing tool calls in parallel with the actor model's reasoning.

## Core Idea

**Traditional Sequential Execution:**
```
Time: 0s                    10s                    12s
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      Actor thinking...     Execute tool
                            â–“â–“
      Total: 12s
```

**Speculative Parallel Execution:**
```
Time: 0s            2s      4s                     10s    10.001s
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
      Actor:        â”‚                              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    Thinking...

      Spec:   Predict Execute
              â–“â–“      â–“â–“
                      â””â”€â†’ Cache result

                                                   Check cache
                                                   âš¡ Match! Use cached result
                                                   (Tool execution skipped)

      Total: ~10s (2s saved, 17% speedup)
```

**Key insight**: While the actor model (GPT-5) is thinking, a lightweight spec model (GPT-5-mini) predicts and pre-executes the next tool call. If the prediction matches, we use the cached result and skip tool execution entirely.

## Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      START          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     node_llm        â”‚
                    â”‚                     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ Actor Model   â”‚  â”‚  Run in parallel
                    â”‚  â”‚ (GPT-5)       â”‚  â”‚  
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â”‚         +           â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ Spec Model    â”‚  â”‚
                    â”‚  â”‚ (GPT-5-mini)  â”‚  â”‚
                    â”‚  â”‚ + Pre-execute â”‚  â”‚
                    â”‚  â”‚ + Cache       â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”Œâ”€â”€â”€â”€â†’â”‚    node_tools       â”‚
              â”‚     â”‚                     â”‚
              â”‚     â”‚  Check cache:       â”‚
              â”‚     â”‚  - Match? Use cache â”‚
              â”‚     â”‚  - No? Execute      â”‚
              â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                â”‚
              â”‚                â–¼
              â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     â”‚   should_end?       â”‚
              â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                â”‚
              â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
              â”‚         â”‚             â”‚
              â”‚       Done       Continue
              â”‚         â”‚             â”‚
              â”‚         â–¼             â”‚
              â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”         â”‚
              â”‚     â”‚  END  â”‚         â”‚
              â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
              â”‚                       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Download GAIA dataset
python3 download_gaia.py
```

### Option 1: Using OpenAI Models

```bash
# Set API keys
export OPENAI_API_KEY="your-openai-key"
export SERPER_API_KEY="your-serper-key"  # For web search

# Configure models (optional, defaults shown)
export MODEL_PROVIDER="openai"
export GAIA_ACTOR_MODEL="gpt-5"
export GAIA_SPEC_MODEL="gpt-5-mini"
```

### Option 2: Using Open-Source Models via vLLM

```bash
# Start vLLM server with tool calling support
CUDA_VISIBLE_DEVICES=7 vllm serve /path/to/your/model \
    --port 8003 \
    --host 0.0.0.0 \
    --enable-auto-tool-choice \
    --tool-call-parser hermes

CUDA_VISIBLE_DEVICES=6 vllm serve /home/nvidia/data/models/Qwen2.5-3B-Instruct \
    --port 8004 \
    --host 0.0.0.0 \
    --enable-auto-tool-choice \
    --tool-call-parser hermes

# Configure to use vLLM
export MODEL_PROVIDER="vllm"
export VLLM_BASE_URL="http://localhost:8003/v1"
export GAIA_ACTOR_MODEL="/path/to/your/model"
export GAIA_SPEC_MODEL="/path/to/your/model"
export SERPER_API_KEY="your-serper-key"  # For web search
```

**Supported vLLM models:**
- Qwen/Qwen2.5-* (recommended, uses `--tool-call-parser hermes`)
- Qwen/QwQ-32B
- meta-llama/Llama-3.1-* (use `--tool-call-parser llama3_json`)
- See [vLLM Tool Calling docs](https://docs.vllm.ai/en/stable/features/tool_calling/) for more

## Running Evaluations

### Single Example (Detailed)

```bash
# Run specific example
python eval.py gaia_dataset/level1/example_000

# With speculation enabled (default)
python eval.py gaia_dataset/level1/example_000

# Baseline (no speculation)
DISABLE_SPECULATION=1 python eval.py gaia_dataset/level1/example_000
```

### Batch Evaluation (Multiple Examples)

```bash
# Run all level 1 examples with speculation
python eval.py --batch --level 1

# Run first 10 examples
python eval.py --batch --level 1 --max 10

# Baseline (no speculation)
DISABLE_SPECULATION=1 python eval.py --batch --level 1 --max 10

# inlcusive start, exclusive end
python eval.py --batch --level 1 --start 10 --end 21

# Custom output directory
python eval.py --batch --level 1 --output my_results/
```

## Configuration

All configuration is via environment variables (see `env.example` for full list):

```bash
# Model Provider
export MODEL_PROVIDER="openai"            # "openai" or "vllm"

# Models (format depends on provider)
export GAIA_ACTOR_MODEL="gpt-5"           # Main reasoning model
export GAIA_SPEC_MODEL="gpt-5-mini"       # Speculation model (smaller/faster)

# vLLM Settings (only if MODEL_PROVIDER=vllm)
export VLLM_ACTOR_URL="http://localhost:8003/v1"  # Actor model endpoint
export VLLM_SPEC_URL="http://localhost:8004/v1"   # Spec model endpoint

# Execution Limits
export GAIA_MAX_STEPS="15"                # Max reasoning steps

# Speculation
export DISABLE_SPECULATION="1"            # Set to "1" to disable (baseline mode)
export VERIFICATION_STRATEGY="exact"      # "exact", "tool_name_only", or "none"
```

## Output

**Single mode** shows detailed step-by-step execution:
```
[Step 1] LLM
â±ï¸  LLM call: 8.5s

ğŸ”§ Tool Call:
   Tool: search_with_content
   Args: query = Moon perigee...

[Step 1] TOOLS
â±ï¸  Execution: 0.001s

âœ… USING CACHED RESULT (pre-executed by spec model)
   Saved 1.25s from cache
```

**Batch mode** shows compact progress:
```
[1/10] example_000
âœ… CORRECT (89.2s)

[2/10] example_001
âŒ WRONG (102.3s)

...

FINAL RESULTS
Accuracy:  8/10 (80.0%)
Avg Time:  95.4s
```

## Results

Results are saved to JSON files:

```json
{
  "task_id": "e1fc63a2-...",
  "question": "If Eliud Kipchoge could maintain...",
  "ground_truth": "17",
  "predicted": "17",
  "correct": true,
  "steps": 6,
  "time_seconds": 89.2,
  "spec_hits": 5,
  "spec_misses": 0,
  "spec_predictions": 5
}
```