#!/usr/bin/env python3
"""Compare two evaluation result directories."""
import json
import sys
from pathlib import Path
from typing import Dict, List, Tuple


def load_results(result_dir: Path) -> Dict[str, dict]:
    """Load all result JSON files from a directory.
    
    Returns:
        Dict mapping task_id to result data
    """
    results = {}
    
    if not result_dir.exists():
        print(f"Error: {result_dir} not found")
        return results
    
    for json_file in result_dir.glob("result_*.json"):
        try:
            with open(json_file) as f:
                data = json.load(f)
                task_id = data.get("task_id")
                if task_id:
                    results[task_id] = data
        except Exception as e:
            print(f"Warning: Could not load {json_file}: {e}")
    
    return results


def compare_results(baseline_dir: str, spec_dir: str):
    """Compare baseline and speculation results."""
    
    baseline_path = Path(baseline_dir)
    spec_path = Path(spec_dir)
    
    print("=" * 80)
    print("RESULT COMPARISON")
    print("=" * 80)
    print(f"Baseline: {baseline_dir}")
    print(f"Spec:     {spec_dir}")
    print("=" * 80 + "\n")
    
    # Load results
    baseline_results = load_results(baseline_path)
    spec_results = load_results(spec_path)
    
    print(f"Loaded {len(baseline_results)} baseline results")
    print(f"Loaded {len(spec_results)} speculation results\n")
    
    # Find common task IDs
    common_ids = set(baseline_results.keys()) & set(spec_results.keys())
    
    if not common_ids:
        print("Error: No common task IDs found")
        return
    
    print(f"Found {len(common_ids)} examples in common\n")
    
    # Compare each example
    comparisons = []
    
    for task_id in sorted(common_ids):
        baseline = baseline_results[task_id]
        spec = spec_results[task_id]
        
        comparison = {
            "task_id": task_id,
            "question": baseline.get("question", "")[:80] + "...",
            "baseline_correct": baseline.get("correct", False),
            "spec_correct": spec.get("correct", False),
            "baseline_time": baseline.get("time_seconds", 0),
            "spec_time": spec.get("time_seconds", 0),
            "baseline_steps": baseline.get("steps", 0),
            "spec_steps": spec.get("steps", 0),
            "spec_hits": spec.get("spec_hits", 0),
            "spec_misses": spec.get("spec_misses", 0),
        }
        
        # Calculate speedup
        if comparison["baseline_time"] > 0:
            comparison["speedup"] = (comparison["baseline_time"] - comparison["spec_time"]) / comparison["baseline_time"] * 100
            comparison["time_saved"] = comparison["baseline_time"] - comparison["spec_time"]
        else:
            comparison["speedup"] = 0
            comparison["time_saved"] = 0
        
        comparisons.append(comparison)
    
    # Print per-example comparison
    print("=" * 80)
    print("PER-EXAMPLE COMPARISON")
    print("=" * 80)
    print(f"{'ID':<10} {'Baseline':<15} {'Spec':<15} {'Speedup':<10} {'Steps':<12} {'Hits/Total'}")
    print("-" * 80)
    
    for c in comparisons:
        baseline_status = "‚úì" if c["baseline_correct"] else "‚úó"
        spec_status = "‚úì" if c["spec_correct"] else "‚úó"
        
        baseline_info = f"{baseline_status} {c['baseline_time']:.1f}s"
        spec_info = f"{spec_status} {c['spec_time']:.1f}s"
        speedup = f"{c['speedup']:+.1f}%"
        steps_info = f"{c['baseline_steps']} ‚Üí {c['spec_steps']}"
        
        total_checks = c["spec_hits"] + c["spec_misses"]
        hit_rate = f"{c['spec_hits']}/{total_checks}" if total_checks > 0 else "N/A"
        
        print(f"{c['task_id'][:10]:<10} {baseline_info:<15} {spec_info:<15} {speedup:<10} {steps_info:<12} {hit_rate}")
    
    # Calculate aggregate statistics
    print("\n" + "=" * 80)
    print("AGGREGATE STATISTICS")
    print("=" * 80)
    
    baseline_correct = sum(1 for c in comparisons if c["baseline_correct"])
    spec_correct = sum(1 for c in comparisons if c["spec_correct"])
    total = len(comparisons)
    
    baseline_accuracy = baseline_correct / total * 100
    spec_accuracy = spec_correct / total * 100
    accuracy_diff = spec_accuracy - baseline_accuracy
    
    baseline_avg_time = sum(c["baseline_time"] for c in comparisons) / total
    spec_avg_time = sum(c["spec_time"] for c in comparisons) / total
    avg_speedup = (baseline_avg_time - spec_avg_time) / baseline_avg_time * 100
    
    baseline_total_time = sum(c["baseline_time"] for c in comparisons)
    spec_total_time = sum(c["spec_time"] for c in comparisons)
    total_time_saved = baseline_total_time - spec_total_time
    
    baseline_avg_steps = sum(c["baseline_steps"] for c in comparisons) / total
    spec_avg_steps = sum(c["spec_steps"] for c in comparisons) / total
    steps_diff = spec_avg_steps - baseline_avg_steps
    
    total_hits = sum(c["spec_hits"] for c in comparisons)
    total_misses = sum(c["spec_misses"] for c in comparisons)
    total_checks = total_hits + total_misses
    overall_hit_rate = total_hits / total_checks * 100 if total_checks > 0 else 0
    
    print(f"\nüìä Accuracy:")
    print(f"  Baseline:  {baseline_correct}/{total} ({baseline_accuracy:.1f}%)")
    print(f"  Spec:      {spec_correct}/{total} ({spec_accuracy:.1f}%)")
    print(f"  Difference: {accuracy_diff:+.1f}%")
    
    print(f"\n‚è±Ô∏è  Latency:")
    print(f"  Baseline avg:  {baseline_avg_time:.1f}s")
    print(f"  Spec avg:      {spec_avg_time:.1f}s")
    print(f"  Speedup:       {avg_speedup:+.1f}%")
    print(f"  Time saved:    {total_time_saved:.1f}s total ({total_time_saved/total:.1f}s per example)")
    
    print(f"\nüî¢ Steps:")
    print(f"  Baseline avg:  {baseline_avg_steps:.1f}")
    print(f"  Spec avg:      {spec_avg_steps:.1f}")
    print(f"  Difference:    {steps_diff:+.1f}")
    
    print(f"\nüíæ Cache Performance:")
    print(f"  Hits:      {total_hits}")
    print(f"  Misses:    {total_misses}")
    print(f"  Hit rate:  {overall_hit_rate:.1f}%")
    
    print("\n" + "=" * 80)
    
    # Determine if spec is better
    if spec_accuracy >= baseline_accuracy and avg_speedup > 0:
        print("‚úÖ SPECULATION WINS: Same or better accuracy with lower latency")
    elif spec_accuracy >= baseline_accuracy:
        print("‚ö†Ô∏è  MIXED: Same or better accuracy but higher latency")
    elif avg_speedup > 0:
        print("‚ö†Ô∏è  MIXED: Lower latency but worse accuracy")
    else:
        print("‚ùå BASELINE WINS: Better accuracy and latency")
    
    print("=" * 80 + "\n")


def main():
    """Main entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Compare baseline and speculation results")
    parser.add_argument("baseline_dir", help="Baseline results directory")
    parser.add_argument("spec_dir", help="Speculation results directory")
    
    args = parser.parse_args()
    
    compare_results(args.baseline_dir, args.spec_dir)


if __name__ == "__main__":
    main()

